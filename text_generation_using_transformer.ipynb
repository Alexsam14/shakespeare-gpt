{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "13gGc7NEuPdFsw4stkLVKgOYFIuR8p0HD",
      "authorship_tag": "ABX9TyPr2KFD7L3poVvldt/SlFox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexsam14/shakespeare-gpt/blob/main/text_generation_using_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting data and printing few lines\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "print(text[:100])\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(chars)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix8Ye-_s545H",
        "outputId": "97b60819-ab1b-4018-d759-6740edb5eb21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-04 11:19:24--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-02-04 11:19:25 (133 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#char to integer mapping and vice versa\n",
        "\n",
        "#string to integer dictionary\n",
        "stoi = {}\n",
        "for i in range(vocab_size):\n",
        "  stoi[chars[i]]=i\n",
        "\n",
        "#integer to string dictionary\n",
        "itos = {}\n",
        "for i in range(vocab_size):\n",
        "  itos[i] = chars[i]\n",
        "\n",
        "def encode(s):\n",
        "  s_encoded = []\n",
        "  for i in s:\n",
        "    s_encoded.append(stoi[i])\n",
        "  return s_encoded\n",
        "\n",
        "def decode(i):\n",
        "  i_decoded = ''\n",
        "  for j in i:\n",
        "    i_decoded += itos[j]\n",
        "  return i_decoded"
      ],
      "metadata": {
        "id": "lW-VKGJO6Tjn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hyperparameters():\n",
        "  emb_dim = 64\n",
        "  num_heads = 8\n",
        "  num_blocks = 2\n",
        "  batch_size = 64\n",
        "  learning_rate = 1e-6\n",
        "  num_epochs = 10\n",
        "  context_size = 100\n",
        "  dropout = 0.3\n",
        "  return emb_dim, num_heads, num_blocks, batch_size, learning_rate, num_epochs, context_size, dropout\n",
        "\n",
        "emb_dim, num_heads, num_blocks, batch_size, learning_rate, num_epochs, context_size, dropout = get_hyperparameters()"
      ],
      "metadata": {
        "id": "7iygeHVVvwRW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "#Rotary Position Embedding (RoPE) for relative position encoding for transformer weights\n",
        "def rope(x, theta_base=10000.0):\n",
        "  batch_size, seq_len, emb_dim = x.size()\n",
        "  assert emb_dim % 2 == 0\n",
        "\n",
        "  pos = torch.arange(0, seq_len, dtype=torch.float32, device=x.device)\n",
        "  pos = pos.unsqueeze(0).expand(batch_size, seq_len)\n",
        "\n",
        "  p = torch.arange(1, emb_dim // 2 + 1, dtype=torch.float32, device=x.device)\n",
        "  theta_p = 1.0 / (theta_base ** (2 * (p-1)/ emb_dim))\n",
        "\n",
        "  pos = pos.unsqueeze(-1)\n",
        "  theta = pos * theta_p\n",
        "\n",
        "  sin_theta = torch.sin(theta)\n",
        "  cos_theta = torch.cos(theta)\n",
        "\n",
        "  x1 = x[..., 0::2]\n",
        "  x2 = x[..., 1::2]\n",
        "\n",
        "  x_rotated_1 = x1 * cos_theta - x2 * sin_theta\n",
        "  x_rotated_2 = x1 * sin_theta + x2 * cos_theta\n",
        "\n",
        "  x_rotated = torch.stack((x_rotated_1, x_rotated_2), dim=-1).reshape(batch_size, seq_len, emb_dim)\n",
        "  return x_rotated"
      ],
      "metadata": {
        "id": "fcusWlAPpeue"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, emb_dim, d_h):\n",
        "    super().__init__()\n",
        "    self.W_k = nn.Linear(emb_dim, d_h)\n",
        "    self.W_q = nn.Linear(emb_dim, d_h)\n",
        "    self.W_v = nn.Linear(emb_dim, d_h)\n",
        "    self.d_h = d_h\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    K = self.W_k(x)\n",
        "    V = self.W_v(x)\n",
        "    Q = self.W_q(x)\n",
        "    Q, K = rope(Q), rope(K)\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_h)\n",
        "    if mask is not None:\n",
        "      masked_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "    probs = torch.softmax(masked_scores, -1)\n",
        "    attention_weights = torch.matmul(probs, V)\n",
        "    return attention_weights"
      ],
      "metadata": {
        "id": "f1KkX_rXImvC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G7Lq2Wjt5ExQ"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, emb_dim, num_heads):\n",
        "\n",
        "    super().__init__()\n",
        "    assert emb_dim % num_heads == 0\n",
        "\n",
        "    self.d_h = emb_dim // num_heads\n",
        "    self.heads = nn.ModuleList([Head(emb_dim, self.d_h) for _ in range(num_heads)])\n",
        "    self.W_o = nn.Linear(emb_dim, emb_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    head_outputs = [head(x,mask) for head in self.heads]\n",
        "    multihead_output = torch.cat(head_outputs, dim=-1)\n",
        "    return self.dropout(self.W_o(multihead_output))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  #Just a feedforward network to increase network width\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(emb_dim, emb_dim * 4)\n",
        "    self.fc2 = nn.Linear(emb_dim * 4, emb_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "piEznrX35mTv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, emb_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(emb_dim)\n",
        "    self.norm2 = nn.LayerNorm(emb_dim)\n",
        "    self.mha = MultiheadAttention(emb_dim, num_heads)\n",
        "    self.mlp = MLP(emb_dim)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    norm_output = self.norm1(x)\n",
        "    attention_output = self.mha(norm_output, mask)\n",
        "    x = x + attention_output\n",
        "    norm_output = self.norm2(x)\n",
        "    mlp_output = self.mlp(norm_output)\n",
        "    x = x + mlp_output\n",
        "    return x"
      ],
      "metadata": {
        "id": "bV45aFkdO4rw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, num_heads, num_blocks):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.blocks = nn.ModuleList([DecoderBlock(emb_dim, num_heads) for _ in range(num_blocks)])\n",
        "    self.output = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    _, seq_len, _ = x.size()\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len)).to(device)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x = block(x, mask)\n",
        "\n",
        "    return self.output(x)"
      ],
      "metadata": {
        "id": "CfHu-OppWh6g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "#data split into training, validation and test sets in ratio 8:1:1 respectively\n",
        "m = int(0.8*len(data))\n",
        "n = int(0.1*len(data))\n",
        "\n",
        "train_set = data[:m]\n",
        "validation_set = data[m:m+n]\n",
        "test_set = data[m+n:]\n",
        "\n",
        "print(f'Number of training tokens: {len(train_set)}')\n",
        "print(f'Number of validation tokens: {len(validation_set)}')\n",
        "print(f'Number of testing tokens: {len(test_set)}')\n",
        "print(f'Total number of tokens: {sum([len(train_set),len(validation_set),len(test_set)])}')"
      ],
      "metadata": {
        "id": "Ku_X0OiWuOcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ef8ef3-d01c-41cb-d987-f8b246e8e6b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training tokens: 892315\n",
            "Number of validation tokens: 111539\n",
            "Number of testing tokens: 111540\n",
            "Total number of tokens: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextSequenceGenerator(Dataset):\n",
        "  def __init__(self, data, sequence_length):\n",
        "    self.data = data\n",
        "    self.sequence_length = sequence_length\n",
        "    self.n_sequences = len(data) - sequence_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_sequences\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    #Get input sequence and output sequence (shifted by 1)\n",
        "    input_sequence = self.data[idx:idx+self.sequence_length]\n",
        "    target_sequence = self.data[idx+1:idx+self.sequence_length+1]\n",
        "    return input_sequence, target_sequence"
      ],
      "metadata": {
        "id": "dvB-Da89ZDrG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sxzrctyFY1Tk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextSequenceGenerator(train_set, context_size)\n",
        "validation_dataset = TextSequenceGenerator(validation_set, context_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True, num_workers=0, drop_last=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size = batch_size, shuffle = True, num_workers=0, drop_last=True)"
      ],
      "metadata": {
        "id": "qL0RUp64Hx5p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, validation_loader, num_epochs, learning_rate,\n",
        "                device=None, early_stopping_patience=5, scheduler_patience=2\n",
        "                ):\n",
        "  if device is None:\n",
        "    #Use GPU if available otherwise use CPU instead\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model = model.to(device)\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay=1e-3)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "      optimizer, mode='min', patience=scheduler_patience, factor=0.5, verbose=True\n",
        "      )\n",
        "\n",
        "  total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
        "  print(f'Using device: {device}\\n')\n",
        "\n",
        "  #Early stopping setup\n",
        "  best_val_loss = float('inf')\n",
        "  patience_counter = 0\n",
        "  best_model_state = None\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    #Training phase\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "    train_batches = 0\n",
        "\n",
        "\n",
        "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
        "\n",
        "    for (xb, yb) in  train_pbar:\n",
        "      xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #forward pass\n",
        "      logits = model(xb)\n",
        "      logits = logits.reshape(-1, logits.size(-1))\n",
        "      yb = yb.reshape(-1)\n",
        "\n",
        "      #compute loss and backward pass\n",
        "      loss = criterion(logits,yb)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      #update metrics\n",
        "      total_train_loss += loss.item()\n",
        "      train_batches += 1\n",
        "\n",
        "      #Update progress bar\n",
        "      train_pbar.set_postfix({'loss' : f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_train_loss = total_train_loss/train_batches\n",
        "\n",
        "    #Validation phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      val_pbar = tqdm(validation_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
        "      for xb,yb in val_pbar:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        #Forward pass\n",
        "        val_logits = model(xb)\n",
        "        val_logits = val_logits.reshape(-1, val_logits.size(-1))\n",
        "        yb = yb.reshape(-1)\n",
        "\n",
        "        #Loss computation\n",
        "        val_loss = criterion(val_logits, yb)\n",
        "\n",
        "        #Updating metrics\n",
        "        total_val_loss += val_loss.item()\n",
        "        val_batches += 1\n",
        "\n",
        "        #Update progress bar\n",
        "        val_pbar.set_postfix({'loss': f'{val_loss.item():.4f}'})\n",
        "\n",
        "    avg_val_loss = total_val_loss/val_batches\n",
        "\n",
        "    #Print epoch results\n",
        "    print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
        "    print(f'Average Training Loss: {avg_train_loss}')\n",
        "    print(f'Average Validation Loss: {avg_val_loss}')\n",
        "\n",
        "    #Learning rate scheduling\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    #Early stopping check\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        print(f'New best validation loss: {best_val_loss:.4f}')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f'Early stopping counter: {patience_counter}/{early_stopping_patience}')\n",
        "\n",
        "    if patience_counter >= early_stopping_patience:\n",
        "        print('\\nEarly stopping triggered!')\n",
        "        break\n",
        "\n",
        "  # Load best model\n",
        "  if best_model_state is not None:\n",
        "      model.load_state_dict(best_model_state)\n",
        "      print(f'\\nLoaded best model with validation loss: {best_val_loss:.4f}')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "UVohIGs74inr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecoderLanguageModel(vocab_size, emb_dim, num_heads, num_blocks)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "trained_model = train_model(\n",
        "    model=model, train_loader=train_loader, validation_loader=validation_loader,\n",
        "    num_epochs=num_epochs, learning_rate=learning_rate, early_stopping_patience=5\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3KqaMoZ0AZG",
        "outputId": "a08e31f1-fa84-4bd4-ef98-fba4ad8c6a57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total trainable parameters: 108,353\n",
            "Using device: cuda\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 13940/13940 [10:26<00:00, 22.24it/s, loss=2.9473]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 1741/1741 [00:36<00:00, 47.68it/s, loss=2.9748]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10:\n",
            "Average Training Loss: 3.4227553218132796\n",
            "Average Validation Loss: 2.955871495893808\n",
            "New best validation loss: 2.9559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 13940/13940 [10:44<00:00, 21.64it/s, loss=2.6341]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 1741/1741 [00:41<00:00, 42.23it/s, loss=2.6512]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/10:\n",
            "Average Training Loss: 2.7674330277798678\n",
            "Average Validation Loss: 2.6702205361065543\n",
            "New best validation loss: 2.6702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 13940/13940 [10:54<00:00, 21.28it/s, loss=2.5418]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 1741/1741 [00:37<00:00, 46.80it/s, loss=2.5687]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/10:\n",
            "Average Training Loss: 2.5778654498725575\n",
            "Average Validation Loss: 2.5592361626304614\n",
            "New best validation loss: 2.5592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 13940/13940 [10:46<00:00, 21.55it/s, loss=2.4549]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 1741/1741 [00:37<00:00, 46.61it/s, loss=2.5151]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/10:\n",
            "Average Training Loss: 2.4831055677262066\n",
            "Average Validation Loss: 2.4819452244957447\n",
            "New best validation loss: 2.4819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 13940/13940 [10:44<00:00, 21.62it/s, loss=2.3987]\n",
            "Epoch 5/10 [Val]: 100%|██████████| 1741/1741 [00:38<00:00, 45.20it/s, loss=2.4286]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/10:\n",
            "Average Training Loss: 2.418412963748833\n",
            "Average Validation Loss: 2.4268220088592827\n",
            "New best validation loss: 2.4268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 13940/13940 [10:46<00:00, 21.58it/s, loss=2.3387]\n",
            "Epoch 6/10 [Val]: 100%|██████████| 1741/1741 [00:37<00:00, 46.89it/s, loss=2.3731]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/10:\n",
            "Average Training Loss: 2.371556880518559\n",
            "Average Validation Loss: 2.3826340970330238\n",
            "New best validation loss: 2.3826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 13940/13940 [10:41<00:00, 21.73it/s, loss=2.3174]\n",
            "Epoch 7/10 [Val]: 100%|██████████| 1741/1741 [00:37<00:00, 46.21it/s, loss=2.3320]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/10:\n",
            "Average Training Loss: 2.3297582543734325\n",
            "Average Validation Loss: 2.3404340496150877\n",
            "New best validation loss: 2.3404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 13940/13940 [10:36<00:00, 21.89it/s, loss=2.2450]\n",
            "Epoch 8/10 [Val]: 100%|██████████| 1741/1741 [00:37<00:00, 46.95it/s, loss=2.3365]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/10:\n",
            "Average Training Loss: 2.2919022650424514\n",
            "Average Validation Loss: 2.3024110468141927\n",
            "New best validation loss: 2.3024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 13940/13940 [10:34<00:00, 21.98it/s, loss=2.2495]\n",
            "Epoch 9/10 [Val]: 100%|██████████| 1741/1741 [00:37<00:00, 46.90it/s, loss=2.2877]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/10:\n",
            "Average Training Loss: 2.2562254693792063\n",
            "Average Validation Loss: 2.263692749292942\n",
            "New best validation loss: 2.2637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 13940/13940 [10:31<00:00, 22.08it/s, loss=2.2097]\n",
            "Epoch 10/10 [Val]: 100%|██████████| 1741/1741 [00:36<00:00, 47.52it/s, loss=2.2423]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/10:\n",
            "Average Training Loss: 2.2211287883980204\n",
            "Average Validation Loss: 2.226141378018173\n",
            "New best validation loss: 2.2261\n",
            "\n",
            "Loaded best model with validation loss: 2.2261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_token=None, max_new_tokens=50, temperature=1):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  token_indices = encode(start_token)\n",
        "\n",
        "  input_tensor = torch.tensor([token_indices], dtype=torch.long).to(device)\n",
        "\n",
        "  generated_indices = input_tensor\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "    logits = model(generated_indices)\n",
        "    logits = logits[:,-1,:]/temperature\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    generated_indices = torch.cat((generated_indices, next_token), dim=1)\n",
        "\n",
        "  return generated_indices"
      ],
      "metadata": {
        "id": "rnY1npGQwrvQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 't'\n",
        "\n",
        "#generating output for various temperatures\n",
        "generated_output_t1 = generate_text(\n",
        "    trained_model, start_token = text, max_new_tokens=50, temperature = 1\n",
        "    )\n",
        "print(decode(generated_output_t1[0].cpu().tolist()))\n",
        "\n",
        "\n",
        "generated_output_t0_8 = generate_text(\n",
        "    trained_model, start_token = text, max_new_tokens=50, temperature = 0.8\n",
        "   )\n",
        "print(decode(generated_output_t0_8[0].cpu().tolist()))"
      ],
      "metadata": {
        "id": "ndfc7ggMMTKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd08586e-97ad-4c87-f01f-9c275aeb0fa2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t wit,\n",
            "Breir nofcer abte hif wher thing arond wike \n",
            "tery.\n",
            "\n",
            "RDWARIICEVETD:\n",
            "Gan him a dell she tounesen I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save entire model\n",
        "torch.save(trained_model, '/content/drive/MyDrive/Models/shakespeare-gpt-model.pth')\n",
        "\n",
        "# Save just the state dict (recommended)\n",
        "torch.save(trained_model.state_dict(), 'shakespeare-gpt-model_state.pth')"
      ],
      "metadata": {
        "id": "Zz4f2_vvQPd1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GA2SehAXGbcK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}